---
title: "4.4 - 4.5 The Laplace Approximation"
output:
  html_document:
  workflowr::wflow_html:
    code_folding: hide
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Method**: Use a Gaussian distribution to approximate a distribution (usually a posterior distribution in Bayesian). The mean of the Gaussian distribution is usually the mode (/local maximum) of the target distribution. 

**Steps and derivations**: Given the distribution $p(z)$ is defined as:

$$
p(z) = \frac{1}{Z}f(z),
$$

where $Z = \int f(z) dz$ is the normalization coefficient. We want to find the Laplace approximation to $p(z)$. i.e., find a Gaussian distribution that can approximate $p(z)$. 

+ Step 1: Find the mode $z_0$ of $p(z)$. Equivalently,

$$
p'(z_0) = \frac{df(z)}{dz}|_{z=z_0} = 0.
$$

+ Step 2: Use Taylor expansion on $\text{ln} f(z)$ centered on the mode $z_0$.

$$
\text{ln} f(z) \simeq \text{ln}f(z_0) - \frac{1}{2}A(z-z_0)^2
$$

where 

$$
A = -\frac{d^2}{dz^2} \text{ln}f(z)|_{z=z_0}
$$

+ Step 3: Take the exponential.


$$
f(z) \simeq f(z_0) \text{exp}\{-\frac{A}{2}(z-z_0)^2\}
$$

+ Step 4: Normalization.

$$
q(z) = (\frac{A}{2\pi})^{1/2} \text{exp}\{-\frac{A}{2}(z-z_0)^2\}
$$

Similarly, for M-dimensional space $\boldsymbol{z}$, we have 

$$
q(\boldsymbol{z}) = \frac{|\boldsymbol{A}|^{1/2}}{(2\pi)^{M/2}} \text{exp}\{-\frac{1}{2} (\boldsymbol{z} - \boldsymbol{z_0})^T \boldsymbol{A} (\boldsymbol{z} - \boldsymbol{z_0})\} = \mathcal{N}(\boldsymbol{z}|\boldsymbol{z_0}, \boldsymbol{A}^{-1}),
$$

where $\boldsymbol{A}$ is a M $\times$ M Hessian matrix defined by

$$
\boldsymbol{A} = - \nabla \nabla\text{ln}f(\boldsymbol{z})|_{\boldsymbol{z}=\boldsymbol{z_0}}.
$$

**Remarks about this approximation**:

+ In practice, a mode will typically found by running some form of numerical optimization algorithm. 

+ Laplace approximation is most useful when the number of data points is relatively large. This is because the posterior distribtuion is expected to approximate Gaussian as the number of observed data points increases due to central limit theorem.

+ The serious limitation: it is based purely on the true distribution at a specific value (local maximum). So it can fail to capture important global properties. 


**Application 1: Bayesian logistic regression**:

Exact Bayesian inference of logistic regression is intractable. Consider the problem of Bayesian logistic regression. The prior is 

$$
p(\boldsymbol{w}) = \mathcal{N}(\boldsymbol{w}|\boldsymbol{m}_0, \boldsymbol{S}_0).
$$

The posterior is 

$$
p(\boldsymbol{w}|\boldsymbol{t}) \propto p(\boldsymbol{w})p(\boldsymbol{t}|\boldsymbol{w}).
$$

We apply Laplace approximation to the posterior distribution. Then $\boldsymbol{w}_{\text{MAP}}$ is the mode and the covariance is then given by the inverse of the matrix of second derivatives of the negative log posterior such that 

$$
\boldsymbol{S}_N^{-1} = -\nabla\nabla \text{ln}p(\boldsymbol{w}|\boldsymbol{t}).
$$
The Gaussian approximation to the posterior distribution gives

$$
q(\boldsymbol{w}) = \mathcal{N}(\boldsymbol{w}|\boldsymbol{w}_{\text{MAP}}, \boldsymbol{S}_N)
$$

**Application 2: BIC model comparison**:

Bayesian Information Criterion (BIC):

$$
\text{ln}p(\mathcal{D}) \simeq \text{ln}p(\mathcal{D}|\boldsymbol{\theta}_{\text{MAP}}) - \frac{1}{2}M \text{ln} N
$$

where $N$ is the number of data points, $M$ is the number of parameters in $\boldsymbol{\theta}$. The second term penalizes the complexity.

+ Compared to AIC, BIC penalizes model complexity more heavily. 
